{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedParser:\n",
    "    \"\"\" Parses out rss feeds\"\"\"\n",
    "\n",
    "    def __init__(self, start_date='2022-10-01', end_date='2022-11-01', topic_urls_dict = {}, col_names=[]):\n",
    "        \n",
    "        self.start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        self.end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        self.topic_urls_dict = topic_urls_dict\n",
    "        self.all_dfs = []\n",
    "        self.col_names = col_names\n",
    "        \n",
    "        if not self.col_names:\n",
    "        # These are the common columns I've seen in podcast rss feeds\n",
    "            self.col_names = ['published_parsed',\n",
    "                                'id',\n",
    "                                'links',\n",
    "                                'summary',\n",
    "                                'title',\n",
    "                                'summary_detail',\n",
    "                                'title_detail',\n",
    "                                'published',\n",
    "                                'guidislink']\n",
    "\n",
    "    def _retrieve_rss_feed(self, url, topic=''):\n",
    "        xml_data = feedparser.parse(url)\n",
    "        df = pd.DataFrame(xml_data.entries)\n",
    "        df = self._ensure_columns(df, self.col_names)\n",
    "        df['topic'] = topic\n",
    "        self.all_dfs.append(df)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_columns(df, col_names):\n",
    "        return df[col_names]\n",
    "\n",
    "    def gather_data(self):\n",
    "        assert len(self.topic_urls_dict)\n",
    "\n",
    "        for topic, urls in self.topic_urls_dict.items():\n",
    "            for url in urls:\n",
    "                self._retrieve_rss_feed(url, topic)\n",
    "\n",
    "        final_df = pd.concat(self.all_dfs)\n",
    "        final_df.reset_index(drop=True, inplace=True)\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_urls_dict = {'news' : [\"http://rss.cnn.com/rss/cnn_topstories.rss\",\n",
    "                                \"https://www.huffpost.com/section/front-page/feed?x=1\",\n",
    "                                \"https://feeds.simplecast.com/54nAGcIl\",\n",
    "                                \"https://feeds.feedburner.com/Monocle24TheGlobalist\",\n",
    "                                \"https://www.theguardian.com/news/series/todayinfocus/podcast.xml\"],\n",
    "                'crime' : [\"https://feeds.simplecast.com/qm_9xx0g\",\n",
    "                                \"https://rss.art19.com/morbid-a-true-crime-podcast\",\n",
    "                                \"https://rss.art19.com/erm-mfm\",\n",
    "                                \"https://feeds.megaphone.fm/VMP7924981569\",\n",
    "                                \"https://www.omnycontent.com/d/playlist/d83f52e4-2455-47f4-982e-ab790120b954/82e70870-d45e-4b4c-8e17-ab8600091b59/e20bfe1d-24c8-4809-b7b0-ab8600091b62/podcast.rss\",\n",
    "                                \"https://feeds.megaphone.fm/darknetdiaries\",\n",
    "                                \"https://feeds.simplecast.com/GdzgJRQH\",\n",
    "                                \"https://feeds.simplecast.com/xl36XBC2\",\n",
    "                                \"https://rss.art19.com/dr-death\",\n",
    "                                \"https://podcastfeeds.nbcnews.com/HL4TzgYC\"],\n",
    "                'science' : [\"https://feeds.simplecast.com/FO6kxYGj\",\n",
    "                                \"http://feeds.feedburner.com/radiolab\",\n",
    "                                \"https://media.rss.com/fm/feed.xml\",\n",
    "                                \"https://omnycontent.com/d/playlist/e73c998e-6e60-432f-8610-ae210140c5b1/6EA152C0-9E3A-45DE-8672-AE2F0056B113/D8936746-9E22-4DBA-B762-AE2F0056B126/podcast.rss\",\n",
    "                                \"https://www.thenakedscientists.com/naked_scientists_podcast.xml\"],\n",
    "                'sport' : [\"https://media.rss.com/progressivepodcast/feed.xml\",\n",
    "                                \"https://media.rss.com/wholeninesports/feed.xml\",\n",
    "                                \"https://media.rss.com/moodysportswithdanandzach/feed.xml\",\n",
    "                                \"https://media.rss.com/thejosephvorepodcast/feed.xml\",\n",
    "                                \"https://media.rss.com/wholeshabang/feed.xml\",\n",
    "                                \"https://mcsorleys.barstoolsports.com/feed/pardon-my-take\",\n",
    "                                \"https://feeds.megaphone.fm/ESP2298543312\",\n",
    "                                \"https://feeds.megaphone.fm/ESP3500611186\",\n",
    "                                \"https://feeds.megaphone.fm/ESP3025643506\"]}\n",
    "\n",
    "a_ps = AdvancedParser('2022-10-01', '2022-11-01', topic_urls_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pc_df = a_ps.gather_data()\n",
    "# This is saving off the data\n",
    "# all_pc_df.to_pickle('all_podcast_info.pkl')\n",
    "\n",
    "# Ingesting the pkl file\n",
    "all_pc_df = pd.read_pickle('all_podcast_info.pkl')\n",
    "\n",
    "# Getting rid of rows where summary is not populated\n",
    "all_pc_df.dropna(inplace=True)\n",
    "all_pc_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>id</th>\n",
       "      <th>links</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>summary_detail</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>published</th>\n",
       "      <th>guidislink</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>2424</td>\n",
       "      <td>2424</td>\n",
       "      <td>2424</td>\n",
       "      <td>2424</td>\n",
       "      <td>2424</td>\n",
       "      <td>2424</td>\n",
       "      <td>2424</td>\n",
       "      <td>2424</td>\n",
       "      <td>2424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>3357</td>\n",
       "      <td>3357</td>\n",
       "      <td>3357</td>\n",
       "      <td>3357</td>\n",
       "      <td>3357</td>\n",
       "      <td>3357</td>\n",
       "      <td>3357</td>\n",
       "      <td>3357</td>\n",
       "      <td>3357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>2553</td>\n",
       "      <td>2553</td>\n",
       "      <td>2553</td>\n",
       "      <td>2553</td>\n",
       "      <td>2553</td>\n",
       "      <td>2553</td>\n",
       "      <td>2553</td>\n",
       "      <td>2553</td>\n",
       "      <td>2553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         published_parsed    id  links  summary  title  summary_detail  \\\n",
       "topic                                                                    \n",
       "crime                2424  2424   2424     2424   2424            2424   \n",
       "news                 2169  2169   2169     2169   2169            2169   \n",
       "science              3357  3357   3357     3357   3357            3357   \n",
       "sport                2553  2553   2553     2553   2553            2553   \n",
       "\n",
       "         title_detail  published  guidislink  \n",
       "topic                                         \n",
       "crime            2424       2424        2424  \n",
       "news             2169       2169        2169  \n",
       "science          3357       3357        3357  \n",
       "sport            2553       2553        2553  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see if my data is evenly spread\n",
    "all_pc_df.groupby('topic').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the amount for each topic is not the same, I want to balance this to not influence the model. \n",
    "all_topics_list = []\n",
    "for topic, count in all_pc_df.topic.value_counts().to_dict().items():\n",
    "    all_topics_list.append(all_pc_df[all_pc_df['topic'] == topic])\n",
    "\n",
    "# Grabbing the small amount to use as our sample amount for underfitting\n",
    "smallest_topic_amount = min(all_pc_df.topic.value_counts().to_dict().values())\n",
    "\n",
    "# This is where we will underfit our data\n",
    "temp_dfs = []\n",
    "for temp_df in all_topics_list:\n",
    "    temp_dfs.append(temp_df.sample(smallest_topic_amount))\n",
    "\n",
    "fitted_df = pd.concat(temp_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>id</th>\n",
       "      <th>links</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>summary_detail</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>published</th>\n",
       "      <th>guidislink</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "      <td>2169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         published_parsed    id  links  summary  title  summary_detail  \\\n",
       "topic                                                                    \n",
       "crime                2169  2169   2169     2169   2169            2169   \n",
       "news                 2169  2169   2169     2169   2169            2169   \n",
       "science              2169  2169   2169     2169   2169            2169   \n",
       "sport                2169  2169   2169     2169   2169            2169   \n",
       "\n",
       "         title_detail  published  guidislink  \n",
       "topic                                         \n",
       "crime            2169       2169        2169  \n",
       "news             2169       2169        2169  \n",
       "science          2169       2169        2169  \n",
       "sport            2169       2169        2169  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see if my data is evenly spread\n",
    "fitted_df.groupby('topic').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep target (topic) and ntext\n",
    "cols_to_keep = ['topic', 'summary']\n",
    "train_df = fitted_df[cols_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleam_summary</th>\n",
       "      <th>clean_summary</th>\n",
       "      <th>summary_tokens</th>\n",
       "      <th>summary_tokens_stem</th>\n",
       "      <th>summary_tokens_lemma</th>\n",
       "      <th>summary_tokens_pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>news</td>\n",
       "      <td>President Barack Obama said not to hire him. T...</td>\n",
       "      <td>president barack obama said not to hire him th...</td>\n",
       "      <td>president barack obama said not to hire him th...</td>\n",
       "      <td>[president, barack, obama, said, hire, attorne...</td>\n",
       "      <td>[presid, barack, obama, said, hire, attorney, ...</td>\n",
       "      <td>[president, barack, obama, said, hire, attorne...</td>\n",
       "      <td>[(president, NN), (barack, NN), (obama, NN), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>news</td>\n",
       "      <td>After the divisiveness of the 2016 election, t...</td>\n",
       "      <td>after the divisiveness of the  election the wo...</td>\n",
       "      <td>after the divisiveness of the  election the wo...</td>\n",
       "      <td>[divisiveness, election, womens, march, became...</td>\n",
       "      <td>[divis, elect, women, march, becam, major, sym...</td>\n",
       "      <td>[divisiveness, election, woman, march, became,...</td>\n",
       "      <td>[(divisiveness, JJ), (election, NN), (womens, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>news</td>\n",
       "      <td>This episode contains descriptions of violence...</td>\n",
       "      <td>this episode contains descriptions of violence...</td>\n",
       "      <td>this episode contains descriptions of violence...</td>\n",
       "      <td>[episode, contains, descriptions, violence, su...</td>\n",
       "      <td>[episod, contain, descript, violenc, suicid, a...</td>\n",
       "      <td>[episode, contains, description, violence, sui...</td>\n",
       "      <td>[(episode, NN), (contains, NNS), (descriptions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>news</td>\n",
       "      <td>For the remainder of this week, “The Daily” is...</td>\n",
       "      <td>for the remainder of this week the daily is re...</td>\n",
       "      <td>for the remainder of this week the daily is re...</td>\n",
       "      <td>[remainder, week, daily, revisiting, episodes,...</td>\n",
       "      <td>[remaind, week, daili, revisit, episod, peopl,...</td>\n",
       "      <td>[remainder, week, daily, revisiting, episode, ...</td>\n",
       "      <td>[(remainder, NN), (week, NN), (daily, RB), (re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>news</td>\n",
       "      <td>This week, The Daily is revisiting some of our...</td>\n",
       "      <td>this week the daily is revisiting some of our ...</td>\n",
       "      <td>this week the daily is revisiting some of our ...</td>\n",
       "      <td>[week, daily, revisiting, favorite, episodes, ...</td>\n",
       "      <td>[week, daili, revisit, favorit, episod, year, ...</td>\n",
       "      <td>[week, daily, revisiting, favorite, episode, y...</td>\n",
       "      <td>[(week, NN), (daily, RB), (revisiting, VBG), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic                                            summary  \\\n",
       "1699  news  President Barack Obama said not to hire him. T...   \n",
       "1254  news  After the divisiveness of the 2016 election, t...   \n",
       "421   news  This episode contains descriptions of violence...   \n",
       "804   news  For the remainder of this week, “The Daily” is...   \n",
       "360   news  This week, The Daily is revisiting some of our...   \n",
       "\n",
       "                                          cleam_summary  \\\n",
       "1699  president barack obama said not to hire him th...   \n",
       "1254  after the divisiveness of the  election the wo...   \n",
       "421   this episode contains descriptions of violence...   \n",
       "804   for the remainder of this week the daily is re...   \n",
       "360   this week the daily is revisiting some of our ...   \n",
       "\n",
       "                                          clean_summary  \\\n",
       "1699  president barack obama said not to hire him th...   \n",
       "1254  after the divisiveness of the  election the wo...   \n",
       "421   this episode contains descriptions of violence...   \n",
       "804   for the remainder of this week the daily is re...   \n",
       "360   this week the daily is revisiting some of our ...   \n",
       "\n",
       "                                         summary_tokens  \\\n",
       "1699  [president, barack, obama, said, hire, attorne...   \n",
       "1254  [divisiveness, election, womens, march, became...   \n",
       "421   [episode, contains, descriptions, violence, su...   \n",
       "804   [remainder, week, daily, revisiting, episodes,...   \n",
       "360   [week, daily, revisiting, favorite, episodes, ...   \n",
       "\n",
       "                                    summary_tokens_stem  \\\n",
       "1699  [presid, barack, obama, said, hire, attorney, ...   \n",
       "1254  [divis, elect, women, march, becam, major, sym...   \n",
       "421   [episod, contain, descript, violenc, suicid, a...   \n",
       "804   [remaind, week, daili, revisit, episod, peopl,...   \n",
       "360   [week, daili, revisit, favorit, episod, year, ...   \n",
       "\n",
       "                                   summary_tokens_lemma  \\\n",
       "1699  [president, barack, obama, said, hire, attorne...   \n",
       "1254  [divisiveness, election, woman, march, became,...   \n",
       "421   [episode, contains, description, violence, sui...   \n",
       "804   [remainder, week, daily, revisiting, episode, ...   \n",
       "360   [week, daily, revisiting, favorite, episode, y...   \n",
       "\n",
       "                              summary_tokens_pos_tagged  \n",
       "1699  [(president, NN), (barack, NN), (obama, NN), (...  \n",
       "1254  [(divisiveness, JJ), (election, NN), (womens, ...  \n",
       "421   [(episode, NN), (contains, NNS), (descriptions...  \n",
       "804   [(remainder, NN), (week, NN), (daily, RB), (re...  \n",
       "360   [(week, NN), (daily, RB), (revisiting, VBG), (...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code was found on https://medium.com/p/f2fc1796e8c7\n",
    "#Lets normalize the data\n",
    "\n",
    "def  clean_text(df, text_field, new_text_field_name):\n",
    "    df[new_text_field_name] = df[text_field].str.lower()\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(<.*?>)|(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "clean_df = clean_text(train_df, 'summary', 'clean_summary')\n",
    "clean_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleam_summary</th>\n",
       "      <th>clean_summary</th>\n",
       "      <th>summary_tokens</th>\n",
       "      <th>summary_tokens_stem</th>\n",
       "      <th>summary_tokens_lemma</th>\n",
       "      <th>summary_tokens_pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>science</td>\n",
       "      <td>Since helium-3 can be used to power the fusion...</td>\n",
       "      <td>since helium can be used to power the fusion r...</td>\n",
       "      <td>since helium used power fusion reactors future...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactors,...</td>\n",
       "      <td>[sinc, helium, use, power, fusion, reactor, fu...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactor, ...</td>\n",
       "      <td>[(since, IN), (helium, NN), (used, VBN), (powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6425</th>\n",
       "      <td>science</td>\n",
       "      <td>As we celebrate Chinese New Year and enter the...</td>\n",
       "      <td>as we celebrate chinese new year and enter the...</td>\n",
       "      <td>celebrate chinese new year enter year fire mon...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[celebr, chines, new, year, enter, year, fire,...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[(celebrate, VB), (chinese, JJ), (new, JJ), (y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>science</td>\n",
       "      <td>&lt;p&gt;Why does heredity hold such power over us? ...</td>\n",
       "      <td>why does heredity hold such power over us how ...</td>\n",
       "      <td>heredity hold power us ancients contemplate ge...</td>\n",
       "      <td>[heredity, hold, power, us, ancients, contempl...</td>\n",
       "      <td>[hered, hold, power, us, ancient, contempl, ge...</td>\n",
       "      <td>[heredity, hold, power, u, ancient, contemplat...</td>\n",
       "      <td>[(heredity, NN), (hold, VBP), (power, NN), (us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6923</th>\n",
       "      <td>science</td>\n",
       "      <td>The human brain strives to organize and classi...</td>\n",
       "      <td>the human brain strives to organize and classi...</td>\n",
       "      <td>human brain strives organize classify sensory ...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[human, brain, strive, organ, classifi, sensor...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[(human, JJ), (brain, NN), (strives, NNS), (or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>science</td>\n",
       "      <td>Patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large collective consciousn...</td>\n",
       "      <td>[patient, zero, looms, large, collective, cons...</td>\n",
       "      <td>[patient, zero, loom, larg, collect, conscious...</td>\n",
       "      <td>[patient, zero, loom, large, collective, consc...</td>\n",
       "      <td>[(patient, NN), (zero, NN), (looms, VBZ), (lar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic                                            summary  \\\n",
       "7045  science  Since helium-3 can be used to power the fusion...   \n",
       "6425  science  As we celebrate Chinese New Year and enter the...   \n",
       "6149  science  <p>Why does heredity hold such power over us? ...   \n",
       "6923  science  The human brain strives to organize and classi...   \n",
       "6307  science  Patient zero looms large in our collective con...   \n",
       "\n",
       "                                          cleam_summary  \\\n",
       "7045  since helium can be used to power the fusion r...   \n",
       "6425  as we celebrate chinese new year and enter the...   \n",
       "6149  why does heredity hold such power over us how ...   \n",
       "6923  the human brain strives to organize and classi...   \n",
       "6307  patient zero looms large in our collective con...   \n",
       "\n",
       "                                          clean_summary  \\\n",
       "7045  since helium used power fusion reactors future...   \n",
       "6425  celebrate chinese new year enter year fire mon...   \n",
       "6149  heredity hold power us ancients contemplate ge...   \n",
       "6923  human brain strives organize classify sensory ...   \n",
       "6307  patient zero looms large collective consciousn...   \n",
       "\n",
       "                                         summary_tokens  \\\n",
       "7045  [since, helium, used, power, fusion, reactors,...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, us, ancients, contempl...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, looms, large, collective, cons...   \n",
       "\n",
       "                                    summary_tokens_stem  \\\n",
       "7045  [sinc, helium, use, power, fusion, reactor, fu...   \n",
       "6425  [celebr, chines, new, year, enter, year, fire,...   \n",
       "6149  [hered, hold, power, us, ancient, contempl, ge...   \n",
       "6923  [human, brain, strive, organ, classifi, sensor...   \n",
       "6307  [patient, zero, loom, larg, collect, conscious...   \n",
       "\n",
       "                                   summary_tokens_lemma  \\\n",
       "7045  [since, helium, used, power, fusion, reactor, ...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, u, ancient, contemplat...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, loom, large, collective, consc...   \n",
       "\n",
       "                              summary_tokens_pos_tagged  \n",
       "7045  [(since, IN), (helium, NN), (used, VBN), (powe...  \n",
       "6425  [(celebrate, VB), (chinese, JJ), (new, JJ), (y...  \n",
       "6149  [(heredity, NN), (hold, VBP), (power, NN), (us...  \n",
       "6923  [(human, JJ), (brain, NN), (strives, NNS), (or...  \n",
       "6307  [(patient, NN), (zero, NN), (looms, VBZ), (lar...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting rid of stop words\n",
    "\n",
    "clean_df['clean_summary'] = clean_df['clean_summary'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleam_summary</th>\n",
       "      <th>clean_summary</th>\n",
       "      <th>summary_tokens</th>\n",
       "      <th>summary_tokens_stem</th>\n",
       "      <th>summary_tokens_lemma</th>\n",
       "      <th>summary_tokens_pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>science</td>\n",
       "      <td>Since helium-3 can be used to power the fusion...</td>\n",
       "      <td>since helium can be used to power the fusion r...</td>\n",
       "      <td>since helium used power fusion reactors future...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactors,...</td>\n",
       "      <td>[sinc, helium, use, power, fusion, reactor, fu...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactor, ...</td>\n",
       "      <td>[(since, IN), (helium, NN), (used, VBN), (powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6425</th>\n",
       "      <td>science</td>\n",
       "      <td>As we celebrate Chinese New Year and enter the...</td>\n",
       "      <td>as we celebrate chinese new year and enter the...</td>\n",
       "      <td>celebrate chinese new year enter year fire mon...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[celebr, chines, new, year, enter, year, fire,...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[(celebrate, VB), (chinese, JJ), (new, JJ), (y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>science</td>\n",
       "      <td>&lt;p&gt;Why does heredity hold such power over us? ...</td>\n",
       "      <td>why does heredity hold such power over us how ...</td>\n",
       "      <td>heredity hold power us ancients contemplate ge...</td>\n",
       "      <td>[heredity, hold, power, us, ancients, contempl...</td>\n",
       "      <td>[hered, hold, power, us, ancient, contempl, ge...</td>\n",
       "      <td>[heredity, hold, power, u, ancient, contemplat...</td>\n",
       "      <td>[(heredity, NN), (hold, VBP), (power, NN), (us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6923</th>\n",
       "      <td>science</td>\n",
       "      <td>The human brain strives to organize and classi...</td>\n",
       "      <td>the human brain strives to organize and classi...</td>\n",
       "      <td>human brain strives organize classify sensory ...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[human, brain, strive, organ, classifi, sensor...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[(human, JJ), (brain, NN), (strives, NNS), (or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>science</td>\n",
       "      <td>Patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large collective consciousn...</td>\n",
       "      <td>[patient, zero, looms, large, collective, cons...</td>\n",
       "      <td>[patient, zero, loom, larg, collect, conscious...</td>\n",
       "      <td>[patient, zero, loom, large, collective, consc...</td>\n",
       "      <td>[(patient, NN), (zero, NN), (looms, VBZ), (lar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic                                            summary  \\\n",
       "7045  science  Since helium-3 can be used to power the fusion...   \n",
       "6425  science  As we celebrate Chinese New Year and enter the...   \n",
       "6149  science  <p>Why does heredity hold such power over us? ...   \n",
       "6923  science  The human brain strives to organize and classi...   \n",
       "6307  science  Patient zero looms large in our collective con...   \n",
       "\n",
       "                                          cleam_summary  \\\n",
       "7045  since helium can be used to power the fusion r...   \n",
       "6425  as we celebrate chinese new year and enter the...   \n",
       "6149  why does heredity hold such power over us how ...   \n",
       "6923  the human brain strives to organize and classi...   \n",
       "6307  patient zero looms large in our collective con...   \n",
       "\n",
       "                                          clean_summary  \\\n",
       "7045  since helium used power fusion reactors future...   \n",
       "6425  celebrate chinese new year enter year fire mon...   \n",
       "6149  heredity hold power us ancients contemplate ge...   \n",
       "6923  human brain strives organize classify sensory ...   \n",
       "6307  patient zero looms large collective consciousn...   \n",
       "\n",
       "                                         summary_tokens  \\\n",
       "7045  [since, helium, used, power, fusion, reactors,...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, us, ancients, contempl...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, looms, large, collective, cons...   \n",
       "\n",
       "                                    summary_tokens_stem  \\\n",
       "7045  [sinc, helium, use, power, fusion, reactor, fu...   \n",
       "6425  [celebr, chines, new, year, enter, year, fire,...   \n",
       "6149  [hered, hold, power, us, ancient, contempl, ge...   \n",
       "6923  [human, brain, strive, organ, classifi, sensor...   \n",
       "6307  [patient, zero, loom, larg, collect, conscious...   \n",
       "\n",
       "                                   summary_tokens_lemma  \\\n",
       "7045  [since, helium, used, power, fusion, reactor, ...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, u, ancient, contemplat...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, loom, large, collective, consc...   \n",
       "\n",
       "                              summary_tokens_pos_tagged  \n",
       "7045  [(since, IN), (helium, NN), (used, VBN), (powe...  \n",
       "6425  [(celebrate, VB), (chinese, JJ), (new, JJ), (y...  \n",
       "6149  [(heredity, NN), (hold, VBP), (power, NN), (us...  \n",
       "6923  [(human, JJ), (brain, NN), (strives, NNS), (or...  \n",
       "6307  [(patient, NN), (zero, NN), (looms, VBZ), (lar...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the summaries\n",
    "\n",
    "clean_df['summary_tokens'] = clean_df['clean_summary'].apply(lambda x: word_tokenize(x))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleam_summary</th>\n",
       "      <th>clean_summary</th>\n",
       "      <th>summary_tokens</th>\n",
       "      <th>summary_tokens_stem</th>\n",
       "      <th>summary_tokens_lemma</th>\n",
       "      <th>summary_tokens_pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>science</td>\n",
       "      <td>Since helium-3 can be used to power the fusion...</td>\n",
       "      <td>since helium can be used to power the fusion r...</td>\n",
       "      <td>since helium used power fusion reactors future...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactors,...</td>\n",
       "      <td>[sinc, helium, use, power, fusion, reactor, fu...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactor, ...</td>\n",
       "      <td>[(since, IN), (helium, NN), (used, VBN), (powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6425</th>\n",
       "      <td>science</td>\n",
       "      <td>As we celebrate Chinese New Year and enter the...</td>\n",
       "      <td>as we celebrate chinese new year and enter the...</td>\n",
       "      <td>celebrate chinese new year enter year fire mon...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[celebr, chines, new, year, enter, year, fire,...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[(celebrate, VB), (chinese, JJ), (new, JJ), (y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>science</td>\n",
       "      <td>&lt;p&gt;Why does heredity hold such power over us? ...</td>\n",
       "      <td>why does heredity hold such power over us how ...</td>\n",
       "      <td>heredity hold power us ancients contemplate ge...</td>\n",
       "      <td>[heredity, hold, power, us, ancients, contempl...</td>\n",
       "      <td>[hered, hold, power, us, ancient, contempl, ge...</td>\n",
       "      <td>[heredity, hold, power, u, ancient, contemplat...</td>\n",
       "      <td>[(heredity, NN), (hold, VBP), (power, NN), (us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6923</th>\n",
       "      <td>science</td>\n",
       "      <td>The human brain strives to organize and classi...</td>\n",
       "      <td>the human brain strives to organize and classi...</td>\n",
       "      <td>human brain strives organize classify sensory ...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[human, brain, strive, organ, classifi, sensor...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[(human, JJ), (brain, NN), (strives, NNS), (or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>science</td>\n",
       "      <td>Patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large collective consciousn...</td>\n",
       "      <td>[patient, zero, looms, large, collective, cons...</td>\n",
       "      <td>[patient, zero, loom, larg, collect, conscious...</td>\n",
       "      <td>[patient, zero, loom, large, collective, consc...</td>\n",
       "      <td>[(patient, NN), (zero, NN), (looms, VBZ), (lar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic                                            summary  \\\n",
       "7045  science  Since helium-3 can be used to power the fusion...   \n",
       "6425  science  As we celebrate Chinese New Year and enter the...   \n",
       "6149  science  <p>Why does heredity hold such power over us? ...   \n",
       "6923  science  The human brain strives to organize and classi...   \n",
       "6307  science  Patient zero looms large in our collective con...   \n",
       "\n",
       "                                          cleam_summary  \\\n",
       "7045  since helium can be used to power the fusion r...   \n",
       "6425  as we celebrate chinese new year and enter the...   \n",
       "6149  why does heredity hold such power over us how ...   \n",
       "6923  the human brain strives to organize and classi...   \n",
       "6307  patient zero looms large in our collective con...   \n",
       "\n",
       "                                          clean_summary  \\\n",
       "7045  since helium used power fusion reactors future...   \n",
       "6425  celebrate chinese new year enter year fire mon...   \n",
       "6149  heredity hold power us ancients contemplate ge...   \n",
       "6923  human brain strives organize classify sensory ...   \n",
       "6307  patient zero looms large collective consciousn...   \n",
       "\n",
       "                                         summary_tokens  \\\n",
       "7045  [since, helium, used, power, fusion, reactors,...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, us, ancients, contempl...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, looms, large, collective, cons...   \n",
       "\n",
       "                                    summary_tokens_stem  \\\n",
       "7045  [sinc, helium, use, power, fusion, reactor, fu...   \n",
       "6425  [celebr, chines, new, year, enter, year, fire,...   \n",
       "6149  [hered, hold, power, us, ancient, contempl, ge...   \n",
       "6923  [human, brain, strive, organ, classifi, sensor...   \n",
       "6307  [patient, zero, loom, larg, collect, conscious...   \n",
       "\n",
       "                                   summary_tokens_lemma  \\\n",
       "7045  [since, helium, used, power, fusion, reactor, ...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, u, ancient, contemplat...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, loom, large, collective, consc...   \n",
       "\n",
       "                              summary_tokens_pos_tagged  \n",
       "7045  [(since, IN), (helium, NN), (used, VBN), (powe...  \n",
       "6425  [(celebrate, VB), (chinese, JJ), (new, JJ), (y...  \n",
       "6149  [(heredity, NN), (hold, VBP), (power, NN), (us...  \n",
       "6923  [(human, JJ), (brain, NN), (strives, NNS), (or...  \n",
       "6307  [(patient, NN), (zero, NN), (looms, VBZ), (lar...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming the summary words\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
    "    return stem_text\n",
    "\n",
    "clean_df['summary_tokens_stem'] = clean_df['summary_tokens'].apply(lambda x: word_stemmer(x))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleam_summary</th>\n",
       "      <th>clean_summary</th>\n",
       "      <th>summary_tokens</th>\n",
       "      <th>summary_tokens_stem</th>\n",
       "      <th>summary_tokens_lemma</th>\n",
       "      <th>summary_tokens_pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>science</td>\n",
       "      <td>Since helium-3 can be used to power the fusion...</td>\n",
       "      <td>since helium can be used to power the fusion r...</td>\n",
       "      <td>since helium used power fusion reactors future...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactors,...</td>\n",
       "      <td>[sinc, helium, use, power, fusion, reactor, fu...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactor, ...</td>\n",
       "      <td>[(since, IN), (helium, NN), (used, VBN), (powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6425</th>\n",
       "      <td>science</td>\n",
       "      <td>As we celebrate Chinese New Year and enter the...</td>\n",
       "      <td>as we celebrate chinese new year and enter the...</td>\n",
       "      <td>celebrate chinese new year enter year fire mon...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[celebr, chines, new, year, enter, year, fire,...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[(celebrate, VB), (chinese, JJ), (new, JJ), (y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>science</td>\n",
       "      <td>&lt;p&gt;Why does heredity hold such power over us? ...</td>\n",
       "      <td>why does heredity hold such power over us how ...</td>\n",
       "      <td>heredity hold power us ancients contemplate ge...</td>\n",
       "      <td>[heredity, hold, power, us, ancients, contempl...</td>\n",
       "      <td>[hered, hold, power, us, ancient, contempl, ge...</td>\n",
       "      <td>[heredity, hold, power, u, ancient, contemplat...</td>\n",
       "      <td>[(heredity, NN), (hold, VBP), (power, NN), (us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6923</th>\n",
       "      <td>science</td>\n",
       "      <td>The human brain strives to organize and classi...</td>\n",
       "      <td>the human brain strives to organize and classi...</td>\n",
       "      <td>human brain strives organize classify sensory ...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[human, brain, strive, organ, classifi, sensor...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[(human, JJ), (brain, NN), (strives, NNS), (or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>science</td>\n",
       "      <td>Patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large collective consciousn...</td>\n",
       "      <td>[patient, zero, looms, large, collective, cons...</td>\n",
       "      <td>[patient, zero, loom, larg, collect, conscious...</td>\n",
       "      <td>[patient, zero, loom, large, collective, consc...</td>\n",
       "      <td>[(patient, NN), (zero, NN), (looms, VBZ), (lar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic                                            summary  \\\n",
       "7045  science  Since helium-3 can be used to power the fusion...   \n",
       "6425  science  As we celebrate Chinese New Year and enter the...   \n",
       "6149  science  <p>Why does heredity hold such power over us? ...   \n",
       "6923  science  The human brain strives to organize and classi...   \n",
       "6307  science  Patient zero looms large in our collective con...   \n",
       "\n",
       "                                          cleam_summary  \\\n",
       "7045  since helium can be used to power the fusion r...   \n",
       "6425  as we celebrate chinese new year and enter the...   \n",
       "6149  why does heredity hold such power over us how ...   \n",
       "6923  the human brain strives to organize and classi...   \n",
       "6307  patient zero looms large in our collective con...   \n",
       "\n",
       "                                          clean_summary  \\\n",
       "7045  since helium used power fusion reactors future...   \n",
       "6425  celebrate chinese new year enter year fire mon...   \n",
       "6149  heredity hold power us ancients contemplate ge...   \n",
       "6923  human brain strives organize classify sensory ...   \n",
       "6307  patient zero looms large collective consciousn...   \n",
       "\n",
       "                                         summary_tokens  \\\n",
       "7045  [since, helium, used, power, fusion, reactors,...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, us, ancients, contempl...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, looms, large, collective, cons...   \n",
       "\n",
       "                                    summary_tokens_stem  \\\n",
       "7045  [sinc, helium, use, power, fusion, reactor, fu...   \n",
       "6425  [celebr, chines, new, year, enter, year, fire,...   \n",
       "6149  [hered, hold, power, us, ancient, contempl, ge...   \n",
       "6923  [human, brain, strive, organ, classifi, sensor...   \n",
       "6307  [patient, zero, loom, larg, collect, conscious...   \n",
       "\n",
       "                                   summary_tokens_lemma  \\\n",
       "7045  [since, helium, used, power, fusion, reactor, ...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, u, ancient, contemplat...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, loom, large, collective, consc...   \n",
       "\n",
       "                              summary_tokens_pos_tagged  \n",
       "7045  [(since, IN), (helium, NN), (used, VBN), (powe...  \n",
       "6425  [(celebrate, VB), (chinese, JJ), (new, JJ), (y...  \n",
       "6149  [(heredity, NN), (hold, VBP), (power, NN), (us...  \n",
       "6923  [(human, JJ), (brain, NN), (strives, NNS), (or...  \n",
       "6307  [(patient, NN), (zero, NN), (looms, VBZ), (lar...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the summary words\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "clean_df['summary_tokens_lemma'] = clean_df['summary_tokens'].apply(lambda x: word_lemmatizer(x))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleam_summary</th>\n",
       "      <th>clean_summary</th>\n",
       "      <th>summary_tokens</th>\n",
       "      <th>summary_tokens_stem</th>\n",
       "      <th>summary_tokens_lemma</th>\n",
       "      <th>summary_tokens_pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>science</td>\n",
       "      <td>Since helium-3 can be used to power the fusion...</td>\n",
       "      <td>since helium can be used to power the fusion r...</td>\n",
       "      <td>since helium used power fusion reactors future...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactors,...</td>\n",
       "      <td>[sinc, helium, use, power, fusion, reactor, fu...</td>\n",
       "      <td>[since, helium, used, power, fusion, reactor, ...</td>\n",
       "      <td>[(since, IN), (helium, NN), (used, VBN), (powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6425</th>\n",
       "      <td>science</td>\n",
       "      <td>As we celebrate Chinese New Year and enter the...</td>\n",
       "      <td>as we celebrate chinese new year and enter the...</td>\n",
       "      <td>celebrate chinese new year enter year fire mon...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[celebr, chines, new, year, enter, year, fire,...</td>\n",
       "      <td>[celebrate, chinese, new, year, enter, year, f...</td>\n",
       "      <td>[(celebrate, VB), (chinese, JJ), (new, JJ), (y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>science</td>\n",
       "      <td>&lt;p&gt;Why does heredity hold such power over us? ...</td>\n",
       "      <td>why does heredity hold such power over us how ...</td>\n",
       "      <td>heredity hold power us ancients contemplate ge...</td>\n",
       "      <td>[heredity, hold, power, us, ancients, contempl...</td>\n",
       "      <td>[hered, hold, power, us, ancient, contempl, ge...</td>\n",
       "      <td>[heredity, hold, power, u, ancient, contemplat...</td>\n",
       "      <td>[(heredity, NN), (hold, VBP), (power, NN), (us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6923</th>\n",
       "      <td>science</td>\n",
       "      <td>The human brain strives to organize and classi...</td>\n",
       "      <td>the human brain strives to organize and classi...</td>\n",
       "      <td>human brain strives organize classify sensory ...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[human, brain, strive, organ, classifi, sensor...</td>\n",
       "      <td>[human, brain, strives, organize, classify, se...</td>\n",
       "      <td>[(human, JJ), (brain, NN), (strives, NNS), (or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>science</td>\n",
       "      <td>Patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large in our collective con...</td>\n",
       "      <td>patient zero looms large collective consciousn...</td>\n",
       "      <td>[patient, zero, looms, large, collective, cons...</td>\n",
       "      <td>[patient, zero, loom, larg, collect, conscious...</td>\n",
       "      <td>[patient, zero, loom, large, collective, consc...</td>\n",
       "      <td>[(patient, NN), (zero, NN), (looms, VBZ), (lar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic                                            summary  \\\n",
       "7045  science  Since helium-3 can be used to power the fusion...   \n",
       "6425  science  As we celebrate Chinese New Year and enter the...   \n",
       "6149  science  <p>Why does heredity hold such power over us? ...   \n",
       "6923  science  The human brain strives to organize and classi...   \n",
       "6307  science  Patient zero looms large in our collective con...   \n",
       "\n",
       "                                          cleam_summary  \\\n",
       "7045  since helium can be used to power the fusion r...   \n",
       "6425  as we celebrate chinese new year and enter the...   \n",
       "6149  why does heredity hold such power over us how ...   \n",
       "6923  the human brain strives to organize and classi...   \n",
       "6307  patient zero looms large in our collective con...   \n",
       "\n",
       "                                          clean_summary  \\\n",
       "7045  since helium used power fusion reactors future...   \n",
       "6425  celebrate chinese new year enter year fire mon...   \n",
       "6149  heredity hold power us ancients contemplate ge...   \n",
       "6923  human brain strives organize classify sensory ...   \n",
       "6307  patient zero looms large collective consciousn...   \n",
       "\n",
       "                                         summary_tokens  \\\n",
       "7045  [since, helium, used, power, fusion, reactors,...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, us, ancients, contempl...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, looms, large, collective, cons...   \n",
       "\n",
       "                                    summary_tokens_stem  \\\n",
       "7045  [sinc, helium, use, power, fusion, reactor, fu...   \n",
       "6425  [celebr, chines, new, year, enter, year, fire,...   \n",
       "6149  [hered, hold, power, us, ancient, contempl, ge...   \n",
       "6923  [human, brain, strive, organ, classifi, sensor...   \n",
       "6307  [patient, zero, loom, larg, collect, conscious...   \n",
       "\n",
       "                                   summary_tokens_lemma  \\\n",
       "7045  [since, helium, used, power, fusion, reactor, ...   \n",
       "6425  [celebrate, chinese, new, year, enter, year, f...   \n",
       "6149  [heredity, hold, power, u, ancient, contemplat...   \n",
       "6923  [human, brain, strives, organize, classify, se...   \n",
       "6307  [patient, zero, loom, large, collective, consc...   \n",
       "\n",
       "                              summary_tokens_pos_tagged  \n",
       "7045  [(since, IN), (helium, NN), (used, VBN), (powe...  \n",
       "6425  [(celebrate, VB), (chinese, JJ), (new, JJ), (y...  \n",
       "6149  [(heredity, NN), (hold, VBP), (power, NN), (us...  \n",
       "6923  [(human, JJ), (brain, NN), (strives, NNS), (or...  \n",
       "6307  [(patient, NN), (zero, NN), (looms, VBZ), (lar...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is performing Part of Speech tagging\n",
    "def word_pos_tagger(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text)\n",
    "    return pos_tagged_text\n",
    "\n",
    "clean_df['summary_tokens_pos_tagged'] = clean_df['summary_tokens'].apply(lambda x: word_pos_tagger(x))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also need to map topics to nums (targets)\n",
    "topics = clean_df.topic.unique().tolist()\n",
    "topic_map = dict(zip(topics, list(range(0, len(topics)))))\n",
    "inverse_topic_map = dict(zip(list(range(0, len(topics))), topics))\n",
    "clean_df['topic_num'] = clean_df['topic'].apply(lambda x: topic_map.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used https://towardsdatascience.com/how-to-enter-your-first-kaggle-competition-4717e7b232db as resource\n",
    "# Splitting our data to test and train and creating a model\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df['clean_summary'],clean_df['topic_num'],random_state = 0)\n",
    "pipeline_sgd = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('nb', LogisticRegression()),\n",
    "])\n",
    "model = pipeline_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       537\n",
      "           1       1.00      0.98      0.99       563\n",
      "           2       0.99      0.97      0.98       543\n",
      "           3       0.97      0.97      0.97       526\n",
      "\n",
      "    accuracy                           0.98      2169\n",
      "   macro avg       0.98      0.98      0.98      2169\n",
      "weighted avg       0.98      0.98      0.98      2169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring in different podcasts to test our model more\n",
    "\n",
    "test_topic_urls_dict = {\n",
    "                        'news' : [\"https://feeds.megaphone.fm/slatesthegist\"],\n",
    "                        'crime' : [\"https://audioboom.com/channels/4940872.rss\",\n",
    "                                \"https://rss.art19.com/generation-why-podcast\"],\n",
    "                        'science' : [\"https://feed.theskepticsguide.org/feed/rss.aspx?feed=sgu\"], \n",
    "                        'sport' : [\"https://feeds.megaphone.fm/ESP4820632502\"]\n",
    "}\n",
    "test_parse = AdvancedParser('2022-10-01', '2022-11-01', test_topic_urls_dict)\n",
    "test_df = test_parse.gather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map topics to nums (targets)\n",
    "test_df['topic_num'] = test_df['topic'].apply(lambda x: topic_map.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>id</th>\n",
       "      <th>links</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>summary_detail</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>published</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>summary_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2022, 11, 14, 23, 22, 0, 0, 318, 0)</td>\n",
       "      <td>46e05288-4e2a-11ed-909a-a7bb01bdf4e1</td>\n",
       "      <td>[{'length': '0', 'type': 'audio/mpeg', 'href':...</td>\n",
       "      <td>Princess Martha Louise Norway engaged American...</td>\n",
       "      <td>The Princess And The Shaman (Who Might Also Be...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Mon, 14 Nov 2022 23:22:00 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>news</td>\n",
       "      <td>3</td>\n",
       "      <td>princess martha louise of norway is engaged to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(2022, 11, 12, 22, 30, 0, 5, 316, 0)</td>\n",
       "      <td>45be14ee-4e2a-11ed-909a-9bf573d3c5c0</td>\n",
       "      <td>[{'length': '0', 'type': 'audio/mpeg', 'href':...</td>\n",
       "      <td>In installment Best Of The Gist, look squishie...</td>\n",
       "      <td>BEST OF THE GIST: Squishy Science Edition</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base'...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Sat, 12 Nov 2022 22:30:00 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>news</td>\n",
       "      <td>3</td>\n",
       "      <td>in this installment of best of the gist we loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2022, 11, 11, 22, 0, 0, 4, 315, 0)</td>\n",
       "      <td>460fe80a-4e2a-11ed-909a-fb0f0740e50b</td>\n",
       "      <td>[{'length': '0', 'type': 'audio/mpeg', 'href':...</td>\n",
       "      <td>The pundit, case, The Gist host Mike Pesca. In...</td>\n",
       "      <td>What Went Right For The Dems And Wrong For The...</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base'...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Fri, 11 Nov 2022 22:00:00 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>news</td>\n",
       "      <td>3</td>\n",
       "      <td>the pundit in this case being the gist host mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(2022, 11, 10, 22, 38, 44, 3, 314, 0)</td>\n",
       "      <td>461f40ac-4e2a-11ed-909a-db5ff2299d5f</td>\n",
       "      <td>[{'length': '0', 'type': 'audio/mpeg', 'href':...</td>\n",
       "      <td>As wait … wait … wait results states like Ariz...</td>\n",
       "      <td>Hot-Dog And The Arizona Results</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base'...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Thu, 10 Nov 2022 22:38:44 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>news</td>\n",
       "      <td>3</td>\n",
       "      <td>as we just wait  wait  wait for the results in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(2022, 11, 9, 23, 17, 0, 2, 313, 0)</td>\n",
       "      <td>466f466a-4e2a-11ed-909a-bfc42273a75b</td>\n",
       "      <td>[{'length': '0', 'type': 'audio/mpeg', 'href':...</td>\n",
       "      <td>Californians won’t gambling sports, Oregon leg...</td>\n",
       "      <td>A Red Ripple</td>\n",
       "      <td>{'type': 'text/html', 'language': None, 'base'...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Wed, 09 Nov 2022 23:17:00 -0000</td>\n",
       "      <td>False</td>\n",
       "      <td>news</td>\n",
       "      <td>3</td>\n",
       "      <td>californians wont be gambling on sports oregon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        published_parsed  \\\n",
       "0   (2022, 11, 14, 23, 22, 0, 0, 318, 0)   \n",
       "1   (2022, 11, 12, 22, 30, 0, 5, 316, 0)   \n",
       "2    (2022, 11, 11, 22, 0, 0, 4, 315, 0)   \n",
       "3  (2022, 11, 10, 22, 38, 44, 3, 314, 0)   \n",
       "4    (2022, 11, 9, 23, 17, 0, 2, 313, 0)   \n",
       "\n",
       "                                     id  \\\n",
       "0  46e05288-4e2a-11ed-909a-a7bb01bdf4e1   \n",
       "1  45be14ee-4e2a-11ed-909a-9bf573d3c5c0   \n",
       "2  460fe80a-4e2a-11ed-909a-fb0f0740e50b   \n",
       "3  461f40ac-4e2a-11ed-909a-db5ff2299d5f   \n",
       "4  466f466a-4e2a-11ed-909a-bfc42273a75b   \n",
       "\n",
       "                                               links  \\\n",
       "0  [{'length': '0', 'type': 'audio/mpeg', 'href':...   \n",
       "1  [{'length': '0', 'type': 'audio/mpeg', 'href':...   \n",
       "2  [{'length': '0', 'type': 'audio/mpeg', 'href':...   \n",
       "3  [{'length': '0', 'type': 'audio/mpeg', 'href':...   \n",
       "4  [{'length': '0', 'type': 'audio/mpeg', 'href':...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Princess Martha Louise Norway engaged American...   \n",
       "1  In installment Best Of The Gist, look squishie...   \n",
       "2  The pundit, case, The Gist host Mike Pesca. In...   \n",
       "3  As wait … wait … wait results states like Ariz...   \n",
       "4  Californians won’t gambling sports, Oregon leg...   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Princess And The Shaman (Who Might Also Be...   \n",
       "1          BEST OF THE GIST: Squishy Science Edition   \n",
       "2  What Went Right For The Dems And Wrong For The...   \n",
       "3                    Hot-Dog And The Arizona Results   \n",
       "4                                       A Red Ripple   \n",
       "\n",
       "                                      summary_detail  \\\n",
       "0  {'type': 'text/plain', 'language': None, 'base...   \n",
       "1  {'type': 'text/html', 'language': None, 'base'...   \n",
       "2  {'type': 'text/html', 'language': None, 'base'...   \n",
       "3  {'type': 'text/html', 'language': None, 'base'...   \n",
       "4  {'type': 'text/html', 'language': None, 'base'...   \n",
       "\n",
       "                                        title_detail  \\\n",
       "0  {'type': 'text/plain', 'language': None, 'base...   \n",
       "1  {'type': 'text/plain', 'language': None, 'base...   \n",
       "2  {'type': 'text/plain', 'language': None, 'base...   \n",
       "3  {'type': 'text/plain', 'language': None, 'base...   \n",
       "4  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                         published  guidislink topic  topic_num  \\\n",
       "0  Mon, 14 Nov 2022 23:22:00 -0000       False  news          3   \n",
       "1  Sat, 12 Nov 2022 22:30:00 -0000       False  news          3   \n",
       "2  Fri, 11 Nov 2022 22:00:00 -0000       False  news          3   \n",
       "3  Thu, 10 Nov 2022 22:38:44 -0000       False  news          3   \n",
       "4  Wed, 09 Nov 2022 23:17:00 -0000       False  news          3   \n",
       "\n",
       "                                       summary_clean  \n",
       "0  princess martha louise of norway is engaged to...  \n",
       "1  in this installment of best of the gist we loo...  \n",
       "2  the pundit in this case being the gist host mi...  \n",
       "3  as we just wait  wait  wait for the results in...  \n",
       "4  californians wont be gambling on sports oregon...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_test_clean = test_df.copy()\n",
    "submission_test_clean = clean_text(submission_test_clean, \"summary\", \"summary_clean\")\n",
    "submission_test_clean['summary'] = submission_test_clean['summary'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "submission_test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "submission_test_pred = model.predict(submission_test_clean['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_test_clean['prediction'] = submission_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_test_clean['prediction_topic'] = submission_test_clean['prediction'].apply(lambda x: inverse_topic_map.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>prediction_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>news</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>news</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>news</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>news</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic prediction_topic\n",
       "0   news          science\n",
       "1   news            sport\n",
       "2   news            sport\n",
       "3   news            sport\n",
       "4   news            sport\n",
       "5   news          science\n",
       "6   news            sport\n",
       "7   news            sport\n",
       "8   news            sport\n",
       "9   news            sport\n",
       "10  news          science\n",
       "11  news            sport\n",
       "12  news            sport\n",
       "13  news             news\n",
       "14  news            sport\n",
       "15  news            sport\n",
       "16  news            sport\n",
       "17  news            sport\n",
       "18  news             news\n",
       "19  news          science"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_test_clean[['topic', 'prediction_topic']][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
